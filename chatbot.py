# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VOy0jplQPdUpgVLFCBqocy3msVO9bpSM
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers torch accelerate
!pip install --upgrade gradio
!pip install elasticsearch

"""Vektor veri tabanı için vektör embeddinglerin üretilip saklanabildiği ve arama yapılabildiği Elastic Search tercih edilmiştir. Bunun için gerekli indirme işlemleri sağlanmıştır."""

!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q
!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz
!chown -R daemon:daemon elasticsearch-7.9.2

!huggingface-cli login --token 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

!huggingface-cli whoami

import os
from subprocess import Popen, PIPE, STDOUT

es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],
                     stdout=PIPE, stderr=STDOUT,
                     preexec_fn=lambda: os.setuid(1))  # as daemon

!curl -X GET "localhost:9200/"

import os
import datetime
import uuid
from subprocess import Popen, PIPE, STDOUT
from elasticsearch import Elasticsearch, NotFoundError
from transformers import pipeline, AutoTokenizer
import torch
import gradio as gr

"""Chatbot için kullanılmak üzere açık kaynak bir llm olan Llama 2 modeli tercih edilmiştir."""

# Chatbot modelini yükleme
model = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)
llama_pipeline = pipeline("text-generation", model=model, torch_dtype=torch.float16, device_map="auto")

"""
**Temel düzeyde bir chatbot örneği**"""

def get_response(prompt: str) -> None:
    """
    Generate a response from the Llama model.

    Parameters:
        prompt (str): The user's input/question for the model.

    Returns:
        None: Prints the model's response.
    """
    sequences = llama_pipeline(
        prompt,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=256,
    )
    print("Chatbot:", sequences[0]['generated_text'])

get_response("Hi, I'm Bengisu")

get_response("What's my name?")

"""**Kişiselleştirilmiş cevaplar dönebilen ChatBot'un oluşturulması**"""

# Elasticsearch sunucusunun başlatılması
es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))

# Elasticsearch istemcisini başlatılması
es = Elasticsearch(['http://localhost:9200'])

# Indexleri oluşturma
def create_indices():
    es.indices.create(index='user_profiles', ignore=[400, 404])
    es.indices.create(index='chatbot_history', ignore=[400, 404])

create_indices()

# Yeni bir kullanıcı profili oluşturma
def create_user_profile(user_id):
    with es.options(ignore_status=[404, 409]):
        es.index(index="user_profiles", id=user_id, document={"user_id": user_id, "messages": []})

# Kullanıcı bilgilerini alma
def get_user_info(user_id):
    try:
        return es.get(index="user_profiles", id=user_id)['_source']
    except NotFoundError:
        return None  # Kullanıcı profili bulunamadı

# Mesajları loglama
def log_message(user_id, message, response):
    user_info = get_user_info(user_id)
    if user_info:
        user_messages = user_info.get("messages", [])
        user_messages.append({"message": message, "response": response, "timestamp": datetime.datetime.now()})
        es.index(index="user_profiles", id=user_id, document={"user_id": user_id, "messages": user_messages})

# Chatbot yanıt fonksiyonu
def get_llama_response(message: str, user_id: str) -> str:
    sequences = llama_pipeline(message, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=1024)
    response = sequences[0]['generated_text'].strip()
    log_message(user_id, message, response)
    return response

# Gradio arayüzü
def gradio_interface(input_message, user_id="default_user"):
    if not user_id:
        user_id = "default_user"
    return get_llama_response(input_message, user_id)

# Gradio arayüzünü başlat
gr.Interface(fn=gradio_interface, inputs=["text", "text"], outputs="text").launch()

"""**Alternatif bir kullanım çalışması**"""

#Yeni kullanıcı oluşturma
def create_new_user(user_id):
    doc = {
        'user_id': user_id,
        'messages': []
    }
    es.index(index="user_profiles", id=user_id, document=doc)

# Chatbot etkileşimini kaydetme
def log_chat_interaction(user_id, user_message, chatbot_response):
    # Mevcut kullanıcı profili belgesini alma
    try:
        current_doc = es.get(index="user_profiles", id=user_id)['_source']
    except NotFoundError:
        current_doc = {'user_id': user_id, 'messages': []}

    # Mesaj ve yanıtı belgeye ekleme
    current_doc['messages'].append({'user_message': user_message, 'chatbot_response': chatbot_response})

    # Güncellenmiş belgeyi kaydetme
    es.index(index="user_profiles", id=user_id, document=current_doc)

# Mevcut profili güncelleme
def update_user_profile(user_id, user_data):
    with es.options(ignore_status=[404, 409]):
        es.update(index="user_profiles", id=user_id, body={"doc": user_data, "doc_as_upsert": True})

# Kullanıcı geçmişini sorgulama
def get_user_history(user_id):
    query = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"user_id": user_id}}
                ]
            }
        },
        "sort": [
            {"timestamp": "desc"}
        ],
        "size": 5  # Son 5 mesajı getir
    }
    try:
        res = es.search(index="chatbot_history", body=query)
        return [hit['_source'] for hit in res['hits']['hits']]
    except NotFoundError:
        return []

# Kullanıcı bilgisini chatbot'a entegre etme
def get_llama_response_alternative(message: str, user_id: str) -> str:
    user_info = get_user_info(user_id)
    user_name = user_info.get('name', 'Kullanıcı') if user_info else 'Kullanıcı'

    history = get_user_history(user_id)
    formatted_history = "\n".join([f"{h['message']} - {h['response']}" for h in history])

    prompt = f"{user_name} diyor ki: {message}\n{formatted_history}\nChatbot'un cevabı: "

    # Llama pipeline'ını kullanarak cevap üretme
    sequences = llama_pipeline(prompt, max_length=1024, num_return_sequences=1)
    response = sequences[0]['generated_text'].split('\nChatbot\'un cevabı: ')[-1]

    # Cevap ve mesajı loglama
    log_message(user_id, message, response)
    return response

# Örnek kullanım
user_id = str(uuid.uuid4())
#create_new_user(user_id)

user_message = "Hi, how are you?"
chatbot_response = get_llama_response_alternative(user_message, user_id)

log_chat_interaction(user_id, user_message, chatbot_response)

"""**Elastic Search kullanımı olmadan geçmişi tutabilen bir ChatBot örnek kullanım senaryosu**"""

user_histories = {}

# Mesajları loglama fonksiyonu
def log_message(user_id, message, response):
    if user_id not in user_histories:
        user_histories[user_id] = []
    user_histories[user_id].append((message, response))

def get_user_history(user_id):
    return user_histories.get(user_id, [])

SYSTEM_PROMPT = """<s>[INST] <<SYS>>
You are a helpful bot. Your answers are clear and concise.
<</SYS>>

"""

def format_message(message: str, history: list, memory_limit: int = 3) -> str:
    """
    Formats the message and history for the Llama model.

    Parameters:
        message (str): Current message to send.
        history (list): Past conversation history.
        memory_limit (int): Limit on how many past interactions to consider.

    Returns:
        str: Formatted message string
    """

    if len(history) > memory_limit:
        history = history[-memory_limit:]

    if len(history) == 0:
        return SYSTEM_PROMPT + f"{message} [/INST]"

    formatted_message = SYSTEM_PROMPT + f"{history[0][0]} [/INST] {history[0][1]} </s>"

    for user_msg, model_answer in history[1:]:
        formatted_message += f"<s>[INST] {user_msg} [/INST] {model_answer} </s>"

    formatted_message += f"<s>[INST] {message} [/INST]"

    return formatted_message

def get_llama_response(message: str, history: list) -> str:
    """
    Generates a conversational response from the Llama model.

    Parameters:
        message (str): User's input message.
        history (list): Past conversation history.

    Returns:
        str: Generated response from the Llama model.
    """
    query = format_message(message, history)
    response = ""

    sequences = llama_pipeline(
        query,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=1024,
    )

    generated_text = sequences[0]['generated_text']
    response = generated_text[len(query):]

    print("Chatbot:", response.strip())
    return response.strip()

import gradio as gr

gr.ChatInterface(get_llama_response).launch()