# -*- coding: utf-8 -*-
"""ChatBotChroma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbPBlya6-ip5S2yhWHzLRtZ5UO5ihWYu
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pip install langchain
!pip install pypdf
!pip install sentence_transformers
!pip install chromadb
!pip install accelerate
!pip install --upgrade accelerate
# %pip install bitsandbytes
!pip install transformers torch accelerate
!pip install --upgrade gradio

!huggingface-cli login --token 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

!huggingface-cli whoami

import os
import torch
import chromadb
import gradio as gr
import pandas as pd
from transformers import pipeline, AutoTokenizer
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from chromadb.utils import embedding_functions

"""**Örnek Veri Seti Kullanımı**

Daha önce bir chatbot ile yapılan konuşmalardan oluşan bir veri seti kullanılarak bir konuşma geçmişi oluşturulmuştur. Veri setinin küçük bir kısmı kullanılmıştır.
"""

df = pd.read_csv('/content/drive/MyDrive/topical_chat.csv')
df = df.drop(['sentiment'], axis=1)
df[:20]

"""**Vektör Veri Tabanının Oluşturulması**

Vektör veri tabanı için vektör embeddinglerin üretilip saklanabildiği ve bir sorgu ile bir benzerlik ölçütü kullanılarak benzer embeddinglerin elde edilebildiği Chroma vektör veri tabanı tercih edilmiştir.
"""

CHROMA_DATA_PATH = "/content/drive/MyDrive/chromadb"
EMBED_MODEL = "all-MiniLM-L6-v2"
COLLECTION_NAME = "Chatbotdb"

client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=EMBED_MODEL
)

collection = client.get_or_create_collection(
    name=COLLECTION_NAME,
    embedding_function=embedding_func,
    metadata={"hnsw:space": "cosine"},
)

messages = []
for i in range(len(df[df['conversation_id']==1])):
  messages.append(df['message'][i])

collection.add(
     documents=messages,
     ids=[f"id{i}" for i in range(len(messages))]
 )

"""Burada bir sorgu ile benzer verilerin getirilmesi ile ilgili bir örnek bulunmaktadır."""

query_results = collection.query(
     query_texts=["Find the related topics about google"],
     n_results=5,
 )

query_results.keys()

query_results["documents"]

query_results["distances"]

"""**LLM Modelinin Tanımlanması**

Chatbot için kullanılmak üzere açık kaynak kodlu bir llm olan Llama 2 modeli tercih edilmiştir.
"""

# Chatbot modelini yükleme
model = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)
llama_pipeline = pipeline("text-generation", model=model, torch_dtype=torch.float16, device_map="auto")

SYSTEM_PROMPT = """<s>[INST] <<SYS>>
You are a helpful bot. Your answers are clear and concise.
<</SYS>>

"""

#Kullanıcıdan alınan girdi ve geçmiş konuşmalar kullanılarak sorgu yeni bir formata dönüştürülmüştür.
def format_message(message: str, history: list, memory_limit: int = 5) -> str:
    """
    Formats the message and history for the Llama model.

    Parameters:
        message (str): Current message to send.
        history (list): Past conversation history.
        memory_limit (int): Limit on how many past interactions to consider.

    Returns:
        str: Formatted message string
    """
    if len(history) > memory_limit:
        history = history[-memory_limit:]

    if len(history) == 0:
        return SYSTEM_PROMPT + f"{message} [/INST]"

    formatted_message = SYSTEM_PROMPT + f"{history[0][0]} [/INST] {history[0][1]} </s>"

    for user_msg, model_answer in history[1:]:
        formatted_message += f"<s>[INST] {user_msg} [/INST] {model_answer} </s>"

    formatted_message += f"<s>[INST] {message} [/INST]"

    return formatted_message

#Llama 2 modeli kullanılarak alınan girdi ile cevap döndürülmüştür.
def get_llama_response(message: str, history: list) -> str:
    """
    Generates a conversational response from the Llama model.

    Parameters:
        message (str): User's input message.
        history (list): Past conversation history.

    Returns:
        str: Generated response from the Llama model.
    """
    query = format_message(message, history)
    print(query)
    response = ""

    sequences = llama_pipeline(
        query,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=1024,
    )

    generated_text = sequences[0]['generated_text']
    response = generated_text[len(query):]

    print("Chatbot:", response.strip())
    return response.strip()

"""**Gradio Kullanımı**

Gradio interface'i kullanılarak kullanıcıdan girdi alınarak bir cevap dönen bir arayüz oluşturulmuştur. Burada hem kullanıcıdan alınan girdi hem de chatbot tarafından döndürülen mesaj veri tabanına eklenmiştir.
"""

import uuid
def chat_interface(message):
    collection.add(documents=message, ids=[str(uuid.uuid4())])
    query_results = collection.query(query_texts=message, n_results=5)
    response = get_llama_response(message, query_results["documents"])
    collection.add(documents=response, ids=[str(uuid.uuid4())])
    return response

iface = gr.Interface(fn=chat_interface, inputs=["text"], outputs=[gr.Textbox(label="response", lines=3)])
iface.launch()

#Arayüz olmadan kullanıcıya cevap dönen ve burada benzer sorguların bastırıldığı kod bloğu
flag = True
id = 30
while flag:
  input_text = input("Message:" )
  if input_text=='exit':
    break
  collection.add(
     documents=input_text,
     ids=[str(id)]
  )
  id+=1
  query_results = collection.query(query_texts=input_text, n_results=5)
  print(query_results['documents'])
  response = get_llama_response(input_text, query_results["documents"])
  print(response)
  collection.add(
     documents=response,
     ids=[str(id)]
  )
  id +=1